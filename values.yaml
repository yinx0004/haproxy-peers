# Default values for pxc-cluster.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

finalizers:
  - delete-pxc-pods-in-order
## Set this if you want to delete proxysql persistent volumes on cluster deletion
  - delete-proxysql-pvc
## Set this if you want to delete database persistent volumes on cluster deletion
  - delete-pxc-pvc

nameOverride: ""
fullnameOverride: ""

#operatorImageRepository: percona/percona-xtradb-cluster-operator

crVersion: 1.10.0
pause: false 
initImage: ""
allowUnsafeConfigurations: false 
updateStrategy: RollingUpdate
upgradeOptions:
  versionServiceEndpoint: https://check.percona.com
  #apply: Disabled 
  apply: 8.0-recommended
  schedule: "0 4 * * *"
enableCRValidationWebhook: false
tls: {}
  # SANs:
  #   - pxc-1.example.com
  #   - pxc-2.example.com
  #   - pxc-3.example.com
  # issuerConf:
  #   name: special-selfsigned-issuer
  #   kind: ClusterIssuer
  #   group: cert-manager.io

pxc:
  size: 3
  image:
    repository: percona/percona-xtradb-cluster
    tag: 8.0.22-13.1
  # imagePullPolicy: Always
  autoRecovery: true
  expose:
    enabled: true
    type: ClusterIP 
  #   trafficPolicy: Local
  #   loadBalancerSourceRanges:
  #   - 10.0.0.0/8
    annotations:
      opentack.org/kuryr-subnet-id: fd5b6c6a-550d-4daa-b0b5-031cb358c0f0
      opentack.org/kuryr-svc-ip: 192.168.0.17x
  #     networking.gke.io/load-balancer-type: "Internal"
  # replicationChannels:
  # - name: pxc1_to_pxc2
  #   isSource: true
  # - name: pxc2_to_pxc1
  #   isSource: false
  #   configuration:
  #     sourceRetryCount: 3
  #     sourceConnectRetry: 60
  #   sourcesList:
  #   - host: 10.95.251.101
  #     port: 3306
  #     weight: 100
  # schedulerName: mycustom-scheduler
  imagePullSecrets: []
  # - name: private-registry-credentials
  annotations: 
    k8.v1.cni.cncf.io/networks: pxc-fd85c598a2a9557e-tenant
    opentack.org/kuryr-security-group-id: 6dcfccfa-06da-4acc-aca6-629e28588148,6969b48a-6cdc-4a0c-937e-802bf92be270,5
    opentack.org/kuryr-x-project-id: e91e2679dbd548eb8e49f5c1e7c39f3e
    opentack.org/kuryr-x-routes: 11.64.64.0/20
  #  iam.amazonaws.com/role: role-arn
  labels: {}
  #  rack: rack-22
  # priorityClassName: high-priority
  readinessDelaySec: 15
  livenessDelaySec: 300
  ## Uncomment to pass in a mysql config file
  configuration: |
    #[mysqld]
    #binlog_format                  = row
    #log_bin=/var/lib/mysql/binlog
    #skip_ssl
    #auto_generate_certs            = 0
    #sha256_password_auto_generate_rsa_keys = 0
    #caching_sha2_password_auto_generate_rsa_keys = 0
  #   wsrep_debug=ON
  #   wsrep_provider_options="gcache.size=1G; gcache.recover=yes"
  # envVarsSecret: my-env-var-secrets
  resources:
    requests:
      memory: 1G
      cpu: 100m
    limits: {}
      # memory: 1G
      # cpu: 600m
  # runtimeClassName: image-rc
  sidecars: []
  sidecarResources:
    requests: {}
    limits: {}
  nodeSelector: {}
  #  disktype: ssd
  affinity:
    antiAffinityTopologyKey: "None"
    # advanced:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #       - matchExpressions:
    #         - key: kubernetes.io/e2e-az-name
    #           operator: In
    #           values:
    #           - e2e-az1
    #           - e2e-az2
  tolerations: []
    # - key: "node.alpha.kubernetes.io/unreachable"
    #   operator: "Exists"
    #   effect: "NoExecute"
    #   tolerationSeconds: 6000
  gracePeriod: 600
  podDisruptionBudget:
    # only one of maxUnavailable or minAvaliable can be set
    maxUnavailable: 1
    # minAvailable: 0
  persistence:
    enabled: true 
    # if persistence is enabled, you can specify a hostPath (not recommended)
    # hostPath: /data/mysql
    # otherwise you can specify values for a storage claim (default)
    ## percona data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    storageClass: "local-path"
    accessMode: ReadWriteOnce
    size: 8Gi

  # If you set this to true the cluster will be created without TLS
  disableTLS: false 

  # disable Helm creating TLS certificates if you want to let the operator
  # request certificates from cert-manager
  certManager: true

  # If this is set will not create secrets from values and will instead try to use
  # a pre-existing secret of the same name.
  # clusterSecretName:
  readinessProbes:
    initialDelaySeconds: 15
    timeoutSeconds: 15
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 5
  livenessProbes:
    initialDelaySeconds: 300
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3
  # A custom Kubernetes Security Context for a Container to be used instead of the default one
  # containerSecurityContext:
  #   privileged: false
  # A custom Kubernetes Security Context for a Pod to be used instead of the default one
  # podSecurityContext:
  #   fsGroup: 1001
  #   supplementalGroups:
  #   - 1001
  # serviceAccountName: percona-xtradb-cluster-operator-workload

haproxy:
  enabled: true 
  size: 3
  #image: "percona/percona-xtradb-cluster-operator:1.10.0-haproxy"
  image: "yinx-test-1.instance.cir.sg-sin.sealcloud.com/pxc/myhaproxy:1.10.1"
  replicasServiceEnabled: false 
  imagePullPolicy: IfNotPresent 
  imagePullSecrets: []
  # - name: private-registry-credentials
#  configuration: |
#    global
#      maxconn 2048
#      external-check
#      insecure-fork-wanted
#      stats socket /etc/haproxy/pxc/haproxy.sock mode 600 expose-fd listeners level admin
#
#    defaults
#      default-server init-addr last,libc,none
#      log global
#      mode tcp
#      retries 10
#      timeout client 28800s
#      timeout connect 100500
#      timeout server 28800s
#
#    peers localpeer
#      peer localhost 127.0.0.1:10000
#
#    frontend galera-in
#      bind *:3309 accept-proxy
#      bind *:3306
#      mode tcp
#      option clitcpka
#      default_backend galera-nodes
#
#    frontend galera-admin-in
#      bind *:33062
#      mode tcp
#      option clitcpka
#      default_backend galera-admin-nodes
#
#    frontend galera-replica-in
#      bind *:3307
#      mode tcp
#      option clitcpka
#      default_backend galera-replica-nodes
#
#    frontend galera-mysqlx-in
#      bind *:33060
#      mode tcp
#      option clitcpka
#      default_backend galera-mysqlx-nodes
#
#    frontend stats
#      bind *:8404
#      mode http
#      option http-use-htx
#      http-request use-service prometheus-exporter if { path /metrics }
  annotations: {}
  #  iam.amazonaws.com/role: role-arn
  labels: {}
  #  rack: rack-22
  # serviceType: ClusterIP
  # externalTrafficPolicy: Cluster
  # runtimeClassName: image-rc
  # loadBalancerSourceRanges:
  #   - 10.0.0.0/8
  # serviceAnnotations:
  #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  # priorityClassName: high-priority
  # schedulerName: mycustom-scheduler
  readinessDelaySec: 15
  livenessDelaySec: 300
  envVarsSecret: haproxy-env-var-secrets 
  resources:
    requests:
      memory: 1G
      cpu: 100m
    limits: {}
      # memory: 1G
      # cpu: 600m
  sidecars: 
    - image: "yinx-test-1.instance.cir.sg-sin.sealcloud.com/pxc/myhaproxy:1.10.1" 
      name: haproxy-monit
      command: ["/bin/sh"]
      args: ["/usr/bin/update_haproxy_peers.sh"]
      volumeMounts:
      - mountPath: /etc/haproxy-custom/
        name: haproxy-custom
      - mountPath: /etc/haproxy/pxc 
        name: haproxy-auto
      - mountPath: /etc/mysql/haproxy-env-secret
        name: haproxy-env-var-secrets
##      - mountPath: /etc/mysql/mysql-users-secret
##        name: mysql-users-secret-file
##      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
##        name: kube-api-access-w9lfg
#  sidecarResources:
#    requests: {}
#    limits: {}
  nodeSelector: {}
  #  disktype: ssd
  # serviceAccountName: percona-xtradb-cluster-operator-workload
  affinity:
    antiAffinityTopologyKey: "None"
    # advanced:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #       - matchExpressions:
    #         - key: kubernetes.io/e2e-az-name
    #           operator: In
    #           values:
    #           - e2e-az1
    #           - e2e-az2
  tolerations: []
    # - key: "node.alpha.kubernetes.io/unreachable"
    #   operator: "Exists"
    #   effect: "NoExecute"
    #   tolerationSeconds: 6000
  gracePeriod: 30
  # only one of `maxUnavailable` or `minAvailable` can be set.
  podDisruptionBudget:
    maxUnavailable: 1
    # minAvailable: 0
  readinessProbes:
    initialDelaySeconds: 15
    timeoutSeconds: 1
    periodSeconds: 5
    successThreshold: 1
    failureThreshold: 3
  livenessProbes:
    initialDelaySeconds: 60
    timeoutSeconds: 5
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 4
  # A custom Kubernetes Security Context for a Container to be used instead of the default one
  # containerSecurityContext:
  #   privileged: false
  # A custom Kubernetes Security Context for a Pod to be used instead of the default one
  # podSecurityContext:
  #   fsGroup: 1001
  #   supplementalGroups:
  #   - 1001

proxysql:
  enabled: false
  size: 3
  image: ""
  # imagePullPolicy: Always
  imagePullSecrets: []
#  configuration: |
#    datadir="/var/lib/proxysql"
#
#    admin_variables =
#    {
#      admin_credentials="proxyadmin:admin_password"
#      mysql_ifaces="0.0.0.0:6032"
#      refresh_interval=2000
#
#      cluster_username="proxyadmin"
#      cluster_password="admin_password"
#      checksum_admin_variables=false
#      checksum_ldap_variables=false
#      checksum_mysql_variables=false
#      cluster_check_interval_ms=200
#      cluster_check_status_frequency=100
#      cluster_mysql_query_rules_save_to_disk=true
#      cluster_mysql_servers_save_to_disk=true
#      cluster_mysql_users_save_to_disk=true
#      cluster_proxysql_servers_save_to_disk=true
#      cluster_mysql_query_rules_diffs_before_sync=1
#      cluster_mysql_servers_diffs_before_sync=1
#      cluster_mysql_users_diffs_before_sync=1
#      cluster_proxysql_servers_diffs_before_sync=1
#    }
#
#    mysql_variables=
#    {
#      monitor_password="monitor"
#      monitor_galera_healthcheck_interval=1000
#      threads=2
#      max_connections=2048
#      default_query_delay=0
#      default_query_timeout=10000
#      poll_timeout=2000
#      interfaces="0.0.0.0:3306"
#      default_schema="information_schema"
#      stacksize=1048576
#      connect_timeout_server=10000
#      monitor_history=60000
#      monitor_connect_interval=20000
#      monitor_ping_interval=10000
#      ping_timeout_server=200
#      commands_stats=true
#      sessions_sort=true
#      have_ssl=true
#      ssl_p2s_ca="/etc/proxysql/ssl-internal/ca.crt"
#      ssl_p2s_cert="/etc/proxysql/ssl-internal/tls.crt"
#      ssl_p2s_key="/etc/proxysql/ssl-internal/tls.key"
#      ssl_p2s_cipher="ECDHE-RSA-AES128-GCM-SHA256"
#    }
  # - name: private-registry-credentials
  annotations: {}
  #  iam.amazonaws.com/role: role-arn
  labels: {}
  #  rack: rack-22
  # serviceType: ClusterIP
  # externalTrafficPolicy: Cluster
  # runtimeClassName: image-rc
  # loadBalancerSourceRanges:
  #   - 10.0.0.0/8
  # serviceAnnotations:
  #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
  # priorityClassName: high-priority
  # schedulerName: mycustom-scheduler
  readinessDelaySec: 15
  livenessDelaySec: 300
  # envVarsSecret: my-env-var-secrets
  resources:
    requests:
      memory: 1G
      cpu: 600m
    limits: {}
      # memory: 1G
      # cpu: 600m
  sidecars: []
  sidecarResources:
    requests: {}
    limits: {}
  nodeSelector: {}
  #  disktype: ssd
  # serviceAccountName: percona-xtradb-cluster-operator-workload
  affinity:
    antiAffinityTopologyKey: "None"
    # advanced:
    #   nodeAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       nodeSelectorTerms:
    #       - matchExpressions:
    #         - key: kubernetes.io/e2e-az-name
    #           operator: In
    #           values:
    #           - e2e-az1
    #           - e2e-az2
  tolerations: []
    # - key: "node.alpha.kubernetes.io/unreachable"
    #   operator: "Exists"
    #   effect: "NoExecute"
    #   tolerationSeconds: 6000
  gracePeriod: 30
  # only one of `maxUnavailable` or `minAvailable` can be set.
  podDisruptionBudget:
    maxUnavailable: 1
    # minAvailable: 0
  persistence:
    enabled: true
    # if persistence is enabled, you can specify a hostPath (not recommended)
    # hostPath: /data/mysql
    # otherwise you can specify values for a storage claim (default)
    ## percona data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    accessMode: ReadWriteOnce
    size: 8Gi
  # A custom Kubernetes Security Context for a Container to be used instead of the default one
  # containerSecurityContext:
  #   privileged: false
  # A custom Kubernetes Security Context for a Pod to be used instead of the default one
  # podSecurityContext:
  #   fsGroup: 1001
  #   supplementalGroups:
  #   - 1001

logcollector:
  enabled: true 
  image: ""
  # configuration: |
  #   [OUTPUT]
  #         Name  es
  #         Match *
  #         Host  192.168.2.3
  #         Port  9200
  #         Index my_index
  #         Type  my_type
  resources:
    requests:
      memory: 100M
      cpu: 200m
    limits: {}

pmm:
  enabled: false
  image:
    repository: percona/pmm-client
    tag: 2.23.0
  serverHost: monitoring-service
  serverUser: admin
  resources:
    requests:
      memory: 150M
      cpu: 300m
    limits: {}

backup:
  enabled: false 
  image: "percona/percona-xtradb-cluster-operator:1.10.0-pxc5.7-backup"
  # serviceAccountName: percona-xtradb-cluster-operator
  imagePullSecrets: []
  # - name: private-registry-credentials
  pitr:
    enabled: false 
    storageName: s3-binlogs
    timeBetweenUploads: 60
  storages:
    s3-backup:
      type: s3
      s3:
        bucket: dbbackup 
        #credentialsSecret: my-cluster-name-backup-s3
        credentialsAccessKey: minioadmin
        credentialsSecretKey: minioadmin
        region: us-west-1
        endpointUrl: http://10.10.16.76:9000 
    s3-binlogs:
      type: s3
      s3:
        bucket: binlogbackup/my57-pxc-db 
        #credentialsSecret: my-cluster-name-backup-s3
        credentialsAccessKey: minioadmin
        credentialsSecretKey: minioadmin
        region: us-west-1
        endpointUrl: http://10.10.16.76:9000
  schedule:
    - name: "daily-backup"
      schedule: "55 * * * *"
      keep: 5
      storageName: s3-backup
    # - name: "sat-night-backup"
    #   schedule: "0 0 * * 6"
    #   keep: 3
    #   storageName: s3-us-west

secrets:
  passwords:
    root: insecure-root-password
    xtrabackup: insecure-xtrabackup-password
    monitor: insecure-monitor-password
    clustercheck: insecure-clustercheck-password
    proxyadmin: insecure-proxyadmin-password
    pmmserver: insecure-pmmserver-password
    operator: insecure-operator-password
    replication: insecure-replication-password
  ## If you are using `cert-manager` you can skip this next section.
  tls: {}
    # This should be the name of a secret that contains certificates.
    # it should have the following keys: `ca.crt`, `tls.crt`, `tls.key`
    # If not set the Helm chart will attempt to create certificates
    # for you [not recommended for prod]:
    # cluster:

    # This should be the name of a secret that contains certificates.
    # it should have the following keys: `ca.crt`, `tls.crt`, `tls.key`
    # If not set the Helm chart will attempt to create certificates
    # for you [not recommended for prod]:
    # internal:

